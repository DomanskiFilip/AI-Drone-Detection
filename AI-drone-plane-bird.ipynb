{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbIKo-g5mj5T",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Create the config folder and file so the kaggle CLI works\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"'$KAGGLE_USERNAME'\",\"key\":\"'$KAGGLE_KEY'\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"Successfully configured Kaggle!\")"
      ],
      "metadata": {
        "id": "ZwLsRmgf1X23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLANES VS DRONES VS BIRDS DATASET\n",
        "# Dependencies:\n",
        "import kagglehub\n",
        "import os\n",
        "import cv2 as OpenCV\n",
        "import numpy\n",
        "\n",
        "# Download the dataset\n",
        "dataset_path = kagglehub.dataset_download(\"maryamlsgumel/drone-detection-dataset\")\n",
        "\n",
        "base_path = os.path.join(dataset_path, \"BirdVsDroneVsAirplane\")\n",
        "\n",
        "# Navigate to the folder (update the path if the folder name is different)\n",
        "data_dir = './drone-detection-dataset'\n",
        "\n",
        "# important consts\n",
        "data = []\n",
        "labels = []\n",
        "# hight and width of img after transformation\n",
        "img_width = 64\n",
        "img_height = 64\n",
        "\n",
        "# Mapping folders to labels\n",
        "# !!! category names need to be the same as folder names in the dataset because of how we travverse and translate data from directories!!!\n",
        "categories = {'Birds': 0, 'Drones': 1, 'Aeroplanes': 2}\n",
        "\n",
        "for folder_name, label in categories.items():\n",
        "    # Use the path variable from kagglehub\n",
        "    category_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "    if not os.path.isdir(category_path):\n",
        "        print(f\"Directory not found: {category_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {folder_name}...\")\n",
        "\n",
        "    for img_name in os.listdir(category_path):\n",
        "        # Skip system files and only keep images (skip thumb.db etc.)\n",
        "        if not img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            img_full_path = os.path.join(category_path, img_name)\n",
        "\n",
        "            # Use OpenCV\n",
        "            img = OpenCV.imread(img_full_path)\n",
        "\n",
        "            if img is not None:\n",
        "                # Resize to a standard size\n",
        "                img = OpenCV.resize(img, (img_width, img_height))\n",
        "\n",
        "                # Normalize pixels to range [0, 1]\n",
        "                pixels = img.astype('float32') / 255.0\n",
        "\n",
        "                data.append(pixels)\n",
        "                labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {img_name}: {e}\")\n",
        "\n",
        "data_array = numpy.array(data)\n",
        "labels_array = numpy.array(labels)\n",
        "\n",
        "print(f\"---\")\n",
        "print(f\"Finished! Total images: {len(data_array)}\")\n",
        "print(f\"data_array shape: {data_array.shape}\") # (N, [width], [height], 3)\n",
        "print(f\"labels_array shape: {labels_array.shape}\") # (N,)"
      ],
      "metadata": {
        "id": "GU8usdsyZBN-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display random img to check if eveything worked\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Pick a random index\n",
        "idx = random.randint(0, len(data_array)-1)\n",
        "\n",
        "# Display the image and its label\n",
        "plt.imshow(data_array[idx])\n",
        "plt.title(f\"Label: {labels_array[idx]} (1:Bird, 2:Drone, 3:Plane)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hikBQx1ycYys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we \"flatten\" the data to prepare it for training\n",
        "# from a [width] [height] matrix to a line of numbers\n",
        "# Calculate the total number of features per image\n",
        "# [width] * [height] * 3 = all numbers/parameters in our array\n",
        "# 3 represents primary colors Blue, Red, Green\n",
        "n_samples = data_array.shape[0]\n",
        "n_features = data_array.shape[1] * data_array.shape[2] * data_array.shape[3]\n",
        "\n",
        "# Reshape to (Number of Images, Total Pixels)\n",
        "features = data_array.reshape(n_samples, n_features)\n",
        "\n",
        "print(f\"Original Data Shape: {data_array.shape}\")\n",
        "print(f\"Flattened 'features' Shape:  {features.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Show class distribution\n",
        "unique_labels, counts = numpy.unique(labels, return_counts=True)\n",
        "class_names = {0: \"Birds\", 1: \"Drones\", 2: \"Aeroplanes\"}\n",
        "print(f\"\\nClass distribution:\")\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"  {class_names[label]}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
        "\n",
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain/Test Split:\")\n",
        "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(features)*100:.1f}%)\")\n",
        "print(f\"  Testing samples: {len(X_test)} ({len(X_test)/len(features)*100:.1f}%)\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# importing time for evaluation\n",
        "import time"
      ],
      "metadata": {
        "id": "dwSrRp-124Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function:\n",
        "# Most likely to change when the algorithm does get trained\n",
        "# Pass the rows to the ml algorithms and check if the predicted result is correct, count that up and show % of success\n",
        "def evaluate(model, x, y):\n",
        "\n",
        "  # Use the trained model to predict the class for every row in x\n",
        "  # x = the feature data (images flattened into numbers)\n",
        "  predictions = model.predict(x)\n",
        "\n",
        "  # Compare predictions with the real labels (y)\n",
        "  # (predictions == y) creates an array of True/False values\n",
        "  # numpy.sum counts how many True values (correct matches) there are\n",
        "  correct = numpy.sum(predictions == y)\n",
        "\n",
        "  # Total number of samples tested\n",
        "  total = len(y)\n",
        "\n",
        "  # Accuracy formula: correct predictions divided by total samples\n",
        "  # Multiply by 100 to convert to percentage\n",
        "  accuracy = (correct / total) * 100\n",
        "\n",
        "  # Print evaluation results\n",
        "  print(\"Correct:\", correct)\n",
        "  print(\"Total:\", total)\n",
        "  print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
        "\n",
        "  # Return accuracy so it can also be stored or reused later\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "s9S4ydHHezYB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Algorythm 1 Euclidean Distance: looking at nearest neighbour and predicting as that class\n",
        "class EuclideanDistance:\n",
        "  def __init__(self):\n",
        "      self.X_train = None\n",
        "      self.y_train = None\n",
        "      self.name = \"Euclidean Distance\"\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "        print(f\"\\nTraining {self.name}...\")\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        print(f\"Memorized {len(X_train)} training examples\")\n",
        "\n",
        "  def predict_single(self, x):\n",
        "    # Calculate Euclidean distance to all training samples\n",
        "      # Distance formula: sqrt(sum((x - train_sample)^2))\n",
        "      distances = numpy.sqrt(numpy.sum((self.X_train - x)**2, axis=1))\n",
        "\n",
        "      # Find the index of the smallest distance\n",
        "      closest_index = numpy.argmin(distances)\n",
        "\n",
        "      # Return the label of the closest training sample\n",
        "      return self.y_train[closest_index]\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    predictions = []\n",
        "    for x in X_test:\n",
        "        predictions.append(self.predict_single(x))\n",
        "    return numpy.array(predictions)\n",
        "\n",
        "# Algorythm 2 K Nearest Neighbours: looking at 3 nearest neighbours and predicting as most popular class among them\n",
        "class KNearestNeighbors:\n",
        "  def __init__(self, k=3):\n",
        "        # k: Number of neighbors to vote (default 3)\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "        self.name = f\"K-Nearest Neighbors (k={k})\"\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "      print(f\"\\nTraining {self.name}...\")\n",
        "      self.X_train = numpy.array(X_train)\n",
        "      self.y_train = numpy.array(y_train)\n",
        "      print(f\"Memorized {len(X_train)} training examples\")\n",
        "\n",
        "  def predict_single(self, x):\n",
        "      # Calculate distances\n",
        "      distances = numpy.sqrt(numpy.sum((self.X_train - x)**2, axis=1))\n",
        "\n",
        "      # Get indices of K nearest neighbors\n",
        "      # numpy.argpartition finds K smallest values efficiently\n",
        "      k_indices = numpy.argpartition(distances, self.k)[:self.k].astype(int)\n",
        "\n",
        "      # Get labels of K nearest neighbors\n",
        "      k_labels = self.y_train[k_indices]\n",
        "\n",
        "      # Count votes for each class\n",
        "      # Find which class got the most votes\n",
        "      unique_labels, counts = numpy.unique(k_labels, return_counts=True)\n",
        "      winner = unique_labels[numpy.argmax(counts)]\n",
        "\n",
        "      return winner\n",
        "\n",
        "  def predict(self, X_test):\n",
        "      predictions = []\n",
        "      for x in X_test:\n",
        "          predictions.append(self.predict_single(x))\n",
        "      return numpy.array(predictions)\n",
        "\n",
        "\n",
        "# Algorythm 3 Multi Layer Perceptron: A calssical neural network that trains x ammount of perceprtons over y epochs then predicts based on activated trained features\n",
        "class MultiLayerPerceptron:\n",
        "  def __init__(self, hidden_neurons=100, epochs=30, learning_rate=0.1, random_seed=42):\n",
        "      self.hidden_neurons = hidden_neurons\n",
        "      self.epochs = epochs\n",
        "      self.learning_rate = learning_rate\n",
        "      self.random_seed = random_seed\n",
        "      self.name = f\"Multi-Layer Perceptron ({hidden_neurons} neurons, {epochs} epochs)\"\n",
        "\n",
        "      # These will be set during training\n",
        "      self.weights_input_hidden = None\n",
        "      self.bias_hidden = None\n",
        "      self.weights_hidden_output = None\n",
        "      self.bias_output = None\n",
        "      self.feature_mean = None\n",
        "      self.feature_std = None\n",
        "      self.n_classes = None\n",
        "      self.input_size = None\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      # Sigmoid activation function: squashes values to range (0, 1)\n",
        "      # Formula: 1 / (1 + e^(-x))\n",
        "      return 1.0 / (1.0 + numpy.exp(-numpy.clip(x, -500, 500)))\n",
        "\n",
        "  def sigmoid_derivative(self, activated_value):\n",
        "      # Derivative of sigmoid - used in backpropagation\n",
        "      # Formula: sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "      return activated_value * (1.0 - activated_value)\n",
        "\n",
        "  def normalize_features(self, X):\n",
        "      # Normalize features to have mean=0, std=1\n",
        "      if self.feature_mean is None:\n",
        "          # First time - calculate mean and std from training data\n",
        "          self.feature_mean = numpy.mean(X, axis=0)\n",
        "          self.feature_std = numpy.std(X, axis=0)\n",
        "          # Prevent division by zero\n",
        "          self.feature_std[self.feature_std < 1e-9] = 1.0\n",
        "\n",
        "      # Apply normalization\n",
        "      return (X - self.feature_mean) / self.feature_std\n",
        "\n",
        "  def initialize_weights(self):\n",
        "      # Initialize weights randomly\n",
        "      numpy.random.seed(self.random_seed)\n",
        "      weight_range = 0.1\n",
        "\n",
        "      # Weights from input to hidden layer\n",
        "      self.weights_input_hidden = (numpy.random.rand(self.hidden_neurons, self.input_size) * 2 - 1) * weight_range\n",
        "      self.bias_hidden = (numpy.random.rand(self.hidden_neurons) * 2 - 1) * weight_range\n",
        "\n",
        "      # Weights from hidden to output layer\n",
        "      self.weights_hidden_output = (numpy.random.rand(self.n_classes, self.hidden_neurons) * 2 - 1) * weight_range\n",
        "      self.bias_output = (numpy.random.rand(self.n_classes) * 2 - 1) * weight_range\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "      \"\"\"\n",
        "      Forward pass: push data through the network\n",
        "\n",
        "      Steps:\n",
        "      1. Input -> Hidden layer (with sigmoid activation)\n",
        "      2. Hidden -> Output layer (with sigmoid activation)\n",
        "      \"\"\"\n",
        "      # Input to hidden layer\n",
        "      hidden_sum = numpy.dot(self.weights_input_hidden, x) + self.bias_hidden\n",
        "      hidden_activation = self.sigmoid(hidden_sum)\n",
        "\n",
        "      # Hidden to output layer\n",
        "      output_sum = numpy.dot(self.weights_hidden_output, hidden_activation) + self.bias_output\n",
        "      output_activation = self.sigmoid(output_sum)\n",
        "\n",
        "      return hidden_activation, output_activation\n",
        "\n",
        "  def train(self, X_train, y_train):\n",
        "      \"\"\"\n",
        "      Train the neural network using backpropagation\n",
        "\n",
        "      Backpropagation:\n",
        "      1. Make a prediction (forward pass)\n",
        "      2. Calculate how wrong we were (error)\n",
        "      3. Adjust weights to reduce error (backward pass)\n",
        "      4. Repeat many times\n",
        "      \"\"\"\n",
        "      print(f\"\\nTraining {self.name}...\")\n",
        "      print(\"This may take a minute...\")\n",
        "\n",
        "      # Get dimensions\n",
        "      n_samples, self.input_size = X_train.shape\n",
        "      self.n_classes = len(numpy.unique(y_train))\n",
        "\n",
        "      # Normalize features\n",
        "      X_normalized = self.normalize_features(X_train)\n",
        "\n",
        "      # Initialize weights\n",
        "      self.initialize_weights()\n",
        "\n",
        "      # Training loop\n",
        "      for epoch in range(self.epochs):\n",
        "          correct = 0\n",
        "\n",
        "          # Go through each training example\n",
        "          for i in range(n_samples):\n",
        "              x = X_normalized[i]\n",
        "              target_class = y_train[i]\n",
        "\n",
        "              # FORWARD PASS: Make a prediction\n",
        "              hidden, output = self.forward_pass(x)\n",
        "\n",
        "              # Create target vector (one-hot encoding)\n",
        "              # Example: if target_class=1 and n_classes=3, target=[0, 1, 0]\n",
        "              target = numpy.zeros(self.n_classes)\n",
        "              target[target_class] = 1.0\n",
        "\n",
        "              # CALCULATE ERROR\n",
        "              # Output layer error\n",
        "              output_error = target - output\n",
        "              output_delta = output_error * self.sigmoid_derivative(output)\n",
        "\n",
        "              # Hidden layer error (backpropagate)\n",
        "              hidden_error = numpy.dot(self.weights_hidden_output.T, output_delta)\n",
        "              hidden_delta = hidden_error * self.sigmoid_derivative(hidden)\n",
        "\n",
        "              # UPDATE WEIGHTS (gradient descent)\n",
        "              # Hidden to output weights\n",
        "              self.weights_hidden_output += self.learning_rate * numpy.outer(output_delta, hidden)\n",
        "              self.bias_output += self.learning_rate * output_delta\n",
        "\n",
        "              # Input to hidden weights\n",
        "              self.weights_input_hidden += self.learning_rate * numpy.outer(hidden_delta, x)\n",
        "              self.bias_hidden += self.learning_rate * hidden_delta\n",
        "\n",
        "              # Track accuracy\n",
        "              if numpy.argmax(output) == target_class:\n",
        "                  correct += 1\n",
        "\n",
        "          # Print progress every 10 epochs\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "              accuracy = (correct / n_samples) * 100\n",
        "              print(f\"  Epoch {epoch + 1}/{self.epochs} - Training accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "  def predict_single(self, x):\n",
        "      _, output = self.forward_pass(x)\n",
        "      return numpy.argmax(output)\n",
        "\n",
        "  def predict(self, X_test):\n",
        "      # Normalize test data using training statistics\n",
        "      X_normalized = (X_test - self.feature_mean) / self.feature_std\n",
        "\n",
        "      predictions = []\n",
        "      for x in X_normalized:\n",
        "          predictions.append(self.predict_single(x))\n",
        "\n",
        "      return numpy.array(predictions)\n",
        "\n"
      ],
      "metadata": {
        "id": "LR5ytoXxwkKM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "# Test all algorithms\n",
        "euclidean = EuclideanDistance()\n",
        "euclidean.train(X_train, y_train)\n",
        "results['Euclidean'] = evaluate(euclidean, X_test, y_test)\n",
        "\n",
        "knn3 = KNearestNeighbors(k=3)\n",
        "knn3.train(X_train, y_train)\n",
        "results['KNN-3'] = evaluate(knn3, X_test, y_test)\n",
        "\n",
        "knn5 = KNearestNeighbors(k=5)\n",
        "knn5.train(X_train, y_train)\n",
        "results['KNN-5'] = evaluate(knn5, X_test, y_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(hidden_neurons=100, epochs=30)\n",
        "mlp.train(X_train, y_train)\n",
        "results['MLP'] = evaluate(mlp, X_test, y_test)\n",
        "\n",
        "# Show results\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "for rank, (name, acc) in enumerate(sorted_results, 1):\n",
        "    print(f\"{rank}. {name}: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXglteeO0S7d",
        "outputId": "9c4376f9-0395-4eaa-928d-b71436d8f088"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Euclidean Distance...\n",
            "Memorized 1884 training examples\n",
            "Correct: 392\n",
            "Total: 472\n",
            "Accuracy: 83.05%\n",
            "\n",
            "Training K-Nearest Neighbors (k=3)...\n",
            "Memorized 1884 training examples\n",
            "Correct: 332\n",
            "Total: 472\n",
            "Accuracy: 70.34%\n",
            "\n",
            "Training K-Nearest Neighbors (k=5)...\n",
            "Memorized 1884 training examples\n",
            "Correct: 333\n",
            "Total: 472\n",
            "Accuracy: 70.55%\n",
            "\n",
            "Training Multi-Layer Perceptron (100 neurons, 30 epochs)...\n",
            "This may take a minute...\n",
            "  Epoch 10/30 - Training accuracy: 79.94%\n",
            "  Epoch 20/30 - Training accuracy: 83.17%\n",
            "  Epoch 30/30 - Training accuracy: 85.83%\n",
            "Training complete!\n",
            "Correct: 360\n",
            "Total: 472\n",
            "Accuracy: 76.27%\n",
            "1. Euclidean: 83.05%\n",
            "2. MLP: 76.27%\n",
            "3. KNN-5: 70.55%\n",
            "4. KNN-3: 70.34%\n"
          ]
        }
      ]
    }
  ]
}